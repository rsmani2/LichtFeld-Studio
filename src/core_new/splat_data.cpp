/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 *
 * SPDX-License-Identifier: GPL-3.0-or-later */

#include "core_new/splat_data.hpp"
#include "core_new/logger.hpp"
#include "core_new/parameters.hpp"
#include "core_new/point_cloud.hpp"
#include "external/nanoflann.hpp"

#include <cmath>
#include <expected>
#include <format>
#include <print>
#include <vector>

namespace {

    // Point cloud adaptor for nanoflann
    struct PointCloudAdaptor {
        const float* points;
        size_t num_points;

        PointCloudAdaptor(const float* pts, size_t n)
            : points(pts),
              num_points(n) {}

        inline size_t kdtree_get_point_count() const { return num_points; }

        inline float kdtree_get_pt(const size_t idx, const size_t dim) const {
            return points[idx * 3 + dim];
        }

        template <class BBOX>
        bool kdtree_get_bbox(BBOX& /* bb */) const { return false; }
    };

    using KDTree = nanoflann::KDTreeSingleIndexAdaptor<
        nanoflann::L2_Simple_Adaptor<float, PointCloudAdaptor>,
        PointCloudAdaptor,
        3>;

    /**
     * @brief Compute mean distance to 3 nearest neighbors for each point
     */
    lfs::core::Tensor compute_mean_neighbor_distances(const lfs::core::Tensor& points) {
        auto cpu_points = points.cpu();
        const int num_points = cpu_points.size(0);

        if (cpu_points.ndim() != 2 || cpu_points.size(1) != 3) {
            LOG_ERROR("Input points must have shape [N, 3], got {}", cpu_points.shape().str());
            return lfs::core::Tensor();
        }

        if (cpu_points.dtype() != lfs::core::DataType::Float32) {
            LOG_ERROR("Input points must be float32");
            return lfs::core::Tensor();
        }

        if (num_points <= 1) {
            return lfs::core::Tensor::full({static_cast<size_t>(num_points)}, 0.01f, points.device());
        }

        const float* data = cpu_points.ptr<float>();

        PointCloudAdaptor cloud(data, num_points);
        KDTree index(3, cloud, nanoflann::KDTreeSingleIndexAdaptorParams(10));
        index.buildIndex();

        auto result = lfs::core::Tensor::zeros({static_cast<size_t>(num_points)}, lfs::core::Device::CPU);
        float* result_data = result.ptr<float>();

#pragma omp parallel for if (num_points > 1000)
        for (int i = 0; i < num_points; i++) {
            const float query_pt[3] = {
                data[i * 3 + 0],
                data[i * 3 + 1],
                data[i * 3 + 2]};

            const size_t num_results = std::min(4, num_points);
            std::vector<size_t> ret_indices(num_results);
            std::vector<float> out_dists_sqr(num_results);

            nanoflann::KNNResultSet<float> resultSet(num_results);
            resultSet.init(&ret_indices[0], &out_dists_sqr[0]);
            index.findNeighbors(resultSet, &query_pt[0], nanoflann::SearchParameters(10));

            float sum_dist = 0.0f;
            int valid_neighbors = 0;

            for (size_t j = 0; j < num_results && valid_neighbors < 3; j++) {
                if (out_dists_sqr[j] > 1e-8f) {
                    sum_dist += std::sqrt(out_dists_sqr[j]);
                    valid_neighbors++;
                }
            }

            result_data[i] = (valid_neighbors > 0) ? (sum_dist / valid_neighbors) : 0.01f;
        }

        return result.to(points.device());
    }

} // anonymous namespace

namespace lfs::core {

    // ========== CONSTRUCTOR & DESTRUCTOR ==========

    SplatData::SplatData(int sh_degree,
                         Tensor means_,
                         Tensor sh0_,
                         Tensor shN_,
                         Tensor scaling_,
                         Tensor rotation_,
                         Tensor opacity_,
                         float scene_scale_)
        : _max_sh_degree(sh_degree),
          _active_sh_degree(0), // Start at 0, increases during training to match old behavior
          _scene_scale(scene_scale_),
          _means(std::move(means_)),
          _sh0(std::move(sh0_)),
          _shN(std::move(shN_)),
          _scaling(std::move(scaling_)),
          _rotation(std::move(rotation_)),
          _opacity(std::move(opacity_)) {
    }

    SplatData::~SplatData() = default;

    // ========== MOVE SEMANTICS ==========

    SplatData::SplatData(SplatData&& other) noexcept
        : _active_sh_degree(other._active_sh_degree),
          _max_sh_degree(other._max_sh_degree),
          _scene_scale(other._scene_scale),
          _means(std::move(other._means)),
          _sh0(std::move(other._sh0)),
          _shN(std::move(other._shN)),
          _scaling(std::move(other._scaling)),
          _rotation(std::move(other._rotation)),
          _opacity(std::move(other._opacity)),
          _densification_info(std::move(other._densification_info)) {
        // Reset the moved-from object
        other._active_sh_degree = 0;
        other._max_sh_degree = 0;
        other._scene_scale = 0.0f;
    }

    SplatData& SplatData::operator=(SplatData&& other) noexcept {
        if (this != &other) {
            // Move scalar members
            _active_sh_degree = other._active_sh_degree;
            _max_sh_degree = other._max_sh_degree;
            _scene_scale = other._scene_scale;

            // Move tensors
            _means = std::move(other._means);
            _sh0 = std::move(other._sh0);
            _shN = std::move(other._shN);
            _scaling = std::move(other._scaling);
            _rotation = std::move(other._rotation);
            _opacity = std::move(other._opacity);
            _densification_info = std::move(other._densification_info);
        }
        return *this;
    }

    // ========== COMPUTED GETTERS ==========

    Tensor SplatData::get_means() const {
        return _means;
    }

    Tensor SplatData::get_opacity() const {
        return _opacity.sigmoid().squeeze(-1);
    }

    Tensor SplatData::get_rotation() const {
        // Normalize quaternions along the last dimension
        // _rotation is [N, 4], we want to normalize each quaternion
        // norm = sqrt(sum(x^2)) along dim=1, keepdim=true to get [N, 1]

        auto squared = _rotation.square();
        auto sum_squared = squared.sum({1}, true);    // [N, 1]
        auto norm = sum_squared.sqrt();               // [N, 1]
        return _rotation.div(norm.clamp_min(1e-12f)); // Avoid division by zero
    }

    Tensor SplatData::get_scaling() const {
        return _scaling.exp();
    }

    Tensor SplatData::get_shs() const {
        // _sh0 is [N, 1, 3], _shN is [N, coeffs, 3]
        // Concatenate along dim 1 (coeffs) to get [N, total_coeffs, 3]
        return _sh0.cat(_shN, 1);
    }

    // ========== UTILITY METHODS ==========

    void SplatData::increment_sh_degree() {
        if (_active_sh_degree < _max_sh_degree) {
            _active_sh_degree++;
        }
    }

    void SplatData::set_active_sh_degree(int sh_degree) {
        if (sh_degree <= _max_sh_degree) {
            _active_sh_degree = sh_degree;
        } else {
            _active_sh_degree = _max_sh_degree;
        }
    }

    // ========== GRADIENT MANAGEMENT ==========

    void SplatData::allocate_gradients() {
        // Use empty() instead of zeros() - will be zeroed before first use
        if (_means.is_valid()) {
            _means_grad = Tensor::empty(_means.shape(), _means.device());
        }
        if (_sh0.is_valid()) {
            _sh0_grad = Tensor::empty(_sh0.shape(), _sh0.device());
        }
        if (_shN.is_valid()) {
            _shN_grad = Tensor::empty(_shN.shape(), _shN.device());
        }
        if (_scaling.is_valid()) {
            _scaling_grad = Tensor::empty(_scaling.shape(), _scaling.device());
        }
        if (_rotation.is_valid()) {
            _rotation_grad = Tensor::empty(_rotation.shape(), _rotation.device());
        }
        if (_opacity.is_valid()) {
            _opacity_grad = Tensor::empty(_opacity.shape(), _opacity.device());
        }
    }

    void SplatData::reserve_capacity(size_t capacity) {
        // Reserve capacity for parameters
        if (_means.is_valid()) _means.reserve(capacity);
        if (_sh0.is_valid()) _sh0.reserve(capacity);
        if (_shN.is_valid()) _shN.reserve(capacity);
        if (_scaling.is_valid()) _scaling.reserve(capacity);
        if (_rotation.is_valid()) _rotation.reserve(capacity);
        if (_opacity.is_valid()) _opacity.reserve(capacity);

        // Reserve capacity for gradients (must be same as parameters!)
        if (_means_grad.is_valid()) _means_grad.reserve(capacity);
        if (_sh0_grad.is_valid()) _sh0_grad.reserve(capacity);
        if (_shN_grad.is_valid()) _shN_grad.reserve(capacity);
        if (_scaling_grad.is_valid()) _scaling_grad.reserve(capacity);
        if (_rotation_grad.is_valid()) _rotation_grad.reserve(capacity);
        if (_opacity_grad.is_valid()) _opacity_grad.reserve(capacity);
    }

    void SplatData::zero_gradients() {
        if (_means_grad.is_valid()) {
            _means_grad.zero_();
        }
        if (_sh0_grad.is_valid()) {
            _sh0_grad.zero_();
        }
        if (_shN_grad.is_valid()) {
            _shN_grad.zero_();
        }
        if (_scaling_grad.is_valid()) {
            _scaling_grad.zero_();
        }
        if (_rotation_grad.is_valid()) {
            _rotation_grad.zero_();
        }
        if (_opacity_grad.is_valid()) {
            _opacity_grad.zero_();
        }
    }

    bool SplatData::has_gradients() const {
        return _means_grad.is_valid();
    }

    // ========== FREE FUNCTION: FACTORY ==========

    std::expected<SplatData, std::string> init_model_from_pointcloud(
        const param::TrainingParameters& params,
        Tensor scene_center,
        const PointCloud& pcd,
        int capacity) {

        try {
            LOG_DEBUG("=== init_model_from_pointcloud starting ===");
            LOG_DEBUG("  capacity={}, random={}, sh_degree={}",
                      capacity, params.optimization.random, params.optimization.sh_degree);
            LOG_DEBUG("  scene_center: is_valid={}, device={}, shape={}",
                      scene_center.is_valid(),
                      scene_center.device() == Device::CUDA ? "CUDA" : "CPU",
                      scene_center.shape().str());
            LOG_DEBUG("  pcd.means: is_valid={}, device={}, shape={}, numel={}",
                      pcd.means.is_valid(),
                      pcd.means.device() == Device::CUDA ? "CUDA" : "CPU",
                      pcd.means.shape().str(), pcd.means.numel());
            LOG_DEBUG("  pcd.colors: is_valid={}, device={}, shape={}, numel={}",
                      pcd.colors.is_valid(),
                      pcd.colors.device() == Device::CUDA ? "CUDA" : "CPU",
                      pcd.colors.shape().str(), pcd.colors.numel());

            // Generate positions and colors based on init type
            Tensor positions, colors;

            if (params.optimization.random) {
                const int num_points = params.optimization.init_num_pts;
                const float extent = params.optimization.init_extent;

                LOG_DEBUG("  Using random initialization: num_points={}, extent={}", num_points, extent);
                positions = (Tensor::rand({static_cast<size_t>(num_points), 3}, Device::CUDA)
                                 .mul(2.0f)
                                 .sub(1.0f))
                                .mul(extent);
                colors = Tensor::rand({static_cast<size_t>(num_points), 3}, Device::CUDA);
                LOG_DEBUG("  Random positions created: shape={}, numel={}", positions.shape().str(), positions.numel());
                LOG_DEBUG("  Random colors created: shape={}, numel={}", colors.shape().str(), colors.numel());
            } else {
                LOG_DEBUG("  Using point cloud initialization");
                if (!pcd.means.is_valid() || !pcd.colors.is_valid()) {
                    LOG_ERROR("Point cloud has invalid means or colors: means.is_valid()={}, colors.is_valid()={}",
                              pcd.means.is_valid(), pcd.colors.is_valid());
                    return std::unexpected("Point cloud has invalid means or colors");
                }

                LOG_DEBUG("  Converting pcd.means to CUDA...");
                positions = pcd.means.cuda();
                LOG_DEBUG("  positions after .cuda(): is_valid={}, device={}, ptr={}, shape={}, numel={}",
                          positions.is_valid(),
                          positions.device() == Device::CUDA ? "CUDA" : "CPU",
                          static_cast<void*>(positions.ptr<float>()),
                          positions.shape().str(), positions.numel());

                // Normalize colors from uint8 [0,255] to float32 [0,1] to match old behavior
                LOG_DEBUG("  Converting pcd.colors (dtype={}) to float32...",
                          pcd.colors.dtype() == DataType::UInt8 ? "UInt8" : "Float32");
                colors = pcd.colors.to(DataType::Float32).div(255.0f).cuda();
                LOG_DEBUG("  colors after conversion: is_valid={}, device={}, shape={}, numel={}",
                          colors.is_valid(),
                          colors.device() == Device::CUDA ? "CUDA" : "CPU",
                          colors.shape().str(), colors.numel());
            }

            auto scene_center_device = scene_center.to(positions.device());
            const Tensor dists = positions.sub(scene_center_device).norm(2.0f, {1}, false);

            // Get median distance for scene scale
            auto sorted_dists = dists.sort(0, false);
            const float scene_scale = sorted_dists.first[dists.size(0) / 2].item();

            // RGB to SH conversion (DC component)
            auto rgb_to_sh = [](const Tensor& rgb) {
                constexpr float kInvSH = 0.28209479177387814f;
                return rgb.sub(0.5f).div(kInvSH);
            };

            const size_t num_points = positions.size(0);
            const int64_t feature_shape = static_cast<int64_t>(
                std::pow(params.optimization.sh_degree + 1, 2));

            // Create final tensors first to avoid pool allocations
            Tensor means_, scaling_, rotation_, opacity_, sh0_, shN_;

            if (capacity > 0 && capacity < num_points) {
                LOG_DEBUG("capacity {} was lower than num_points {}.  Matching capacity to points. ", capacity, num_points);
                capacity = num_points;
            }

            if (capacity > 0) {
                LOG_DEBUG("Creating direct tensors with capacity={}", capacity);

                means_ = Tensor::zeros_direct(TensorShape({num_points, 3}), capacity);
                LOG_DEBUG("  means_ allocated: is_valid={}, ptr={}, shape={}, numel={}",
                          means_.is_valid(), static_cast<void*>(means_.ptr<float>()),
                          means_.shape().str(), means_.numel());

                scaling_ = Tensor::zeros_direct(TensorShape({num_points, 3}), capacity);
                LOG_DEBUG("  scaling_ allocated: is_valid={}, ptr={}, shape={}, numel={}",
                          scaling_.is_valid(), static_cast<void*>(scaling_.ptr<float>()),
                          scaling_.shape().str(), scaling_.numel());

                rotation_ = Tensor::zeros_direct(TensorShape({num_points, 4}), capacity);
                LOG_DEBUG("  rotation_ allocated: is_valid={}, ptr={}, shape={}, numel={}",
                          rotation_.is_valid(), static_cast<void*>(rotation_.ptr<float>()),
                          rotation_.shape().str(), rotation_.numel());

                opacity_ = Tensor::zeros_direct(TensorShape({num_points, 1}), capacity);
                LOG_DEBUG("  opacity_ allocated: is_valid={}, ptr={}, shape={}, numel={}",
                          opacity_.is_valid(), static_cast<void*>(opacity_.ptr<float>()),
                          opacity_.shape().str(), opacity_.numel());

                sh0_ = Tensor::zeros_direct(TensorShape({num_points, 1, 3}), capacity);
                LOG_DEBUG("  sh0_ allocated: is_valid={}, ptr={}, shape={}, numel={}",
                          sh0_.is_valid(), static_cast<void*>(sh0_.ptr<float>()),
                          sh0_.shape().str(), sh0_.numel());

                shN_ = Tensor::zeros_direct(TensorShape({num_points, static_cast<size_t>(feature_shape - 1), 3}), capacity);
                LOG_DEBUG("  shN_ allocated: is_valid={}, ptr={}, shape={}, numel={}",
                          shN_.is_valid(), static_cast<void*>(shN_.ptr<float>()),
                          shN_.shape().str(), shN_.numel());

                LOG_DEBUG("Computing and filling values...");
            }

            // Compute parameter values on CPU to avoid pool allocations
            Tensor means_cpu, scaling_cpu, rotation_cpu, opacity_cpu, sh0_cpu, shN_cpu;

            if (capacity > 0) {
                LOG_DEBUG("Computing values on CPU");
                LOG_DEBUG("  positions tensor: is_valid={}, device={}, shape={}, numel={}",
                          positions.is_valid(), positions.device() == Device::CUDA ? "CUDA" : "CPU",
                          positions.shape().str(), positions.numel());

                // Compute means on CPU
                auto positions_cpu = positions.cpu();
                LOG_DEBUG("  positions_cpu after .cpu(): is_valid={}, ptr={}, device={}, shape={}, numel={}",
                          positions_cpu.is_valid(), static_cast<const void*>(positions_cpu.ptr<float>()),
                          positions_cpu.device() == Device::CUDA ? "CUDA" : "CPU",
                          positions_cpu.shape().str(), positions_cpu.numel());

                if (params.optimization.random) {
                    means_cpu = positions_cpu.mul(scene_scale);
                } else {
                    means_cpu = positions_cpu;
                }
                LOG_DEBUG("  means_cpu computed: is_valid={}, ptr={}, device={}, shape={}, numel={}",
                          means_cpu.is_valid(), static_cast<const void*>(means_cpu.ptr<float>()),
                          means_cpu.device() == Device::CUDA ? "CUDA" : "CPU",
                          means_cpu.shape().str(), means_cpu.numel());

                // Compute scaling on CPU
                LOG_DEBUG("  Computing neighbor distances...");
                auto nn_dist = compute_mean_neighbor_distances(means_cpu).clamp_min(1e-7f);
                LOG_DEBUG("  nn_dist computed: is_valid={}, shape={}, numel={}",
                          nn_dist.is_valid(), nn_dist.shape().str(), nn_dist.numel());

                std::vector<int> scale_expand_shape = {static_cast<int>(num_points), 3};
                scaling_cpu = nn_dist.sqrt()
                                  .mul(params.optimization.init_scaling)
                                  .log()
                                  .unsqueeze(-1)
                                  .expand(std::span<const int>(scale_expand_shape));
                LOG_DEBUG("  scaling_cpu computed: is_valid={}, ptr={}, device={}, shape={}, numel={}",
                          scaling_cpu.is_valid(), static_cast<const void*>(scaling_cpu.ptr<float>()),
                          scaling_cpu.device() == Device::CUDA ? "CUDA" : "CPU",
                          scaling_cpu.shape().str(), scaling_cpu.numel());

                // Create identity quaternion rotations on CPU
                LOG_DEBUG("  Creating identity quaternions...");
                rotation_cpu = Tensor::zeros({num_points, 4}, Device::CPU);
                auto rot_acc = rotation_cpu.accessor<float, 2>();
                for (size_t i = 0; i < num_points; i++) {
                    rot_acc(i, 0) = 1.0f;
                }
                LOG_DEBUG("  rotation_cpu created: is_valid={}, ptr={}, shape={}, numel={}",
                          rotation_cpu.is_valid(), static_cast<const void*>(rotation_cpu.ptr<float>()),
                          rotation_cpu.shape().str(), rotation_cpu.numel());

                // Compute opacity on CPU
                LOG_DEBUG("  Computing opacity (init_val={})...", params.optimization.init_opacity);
                auto init_val = params.optimization.init_opacity;
                opacity_cpu = Tensor::full({num_points, 1}, init_val, Device::CPU).logit();
                LOG_DEBUG("  opacity_cpu computed: is_valid={}, ptr={}, shape={}, numel={}",
                          opacity_cpu.is_valid(), static_cast<const void*>(opacity_cpu.ptr<float>()),
                          opacity_cpu.shape().str(), opacity_cpu.numel());

                // Compute SH coefficients on CPU
                LOG_DEBUG("  Computing SH coefficients...");
                LOG_DEBUG("    colors tensor: is_valid={}, device={}, shape={}, numel={}",
                          colors.is_valid(), colors.device() == Device::CUDA ? "CUDA" : "CPU",
                          colors.shape().str(), colors.numel());

                auto colors_cpu = colors.cpu();
                LOG_DEBUG("    colors_cpu: is_valid={}, ptr={}, shape={}, numel={}",
                          colors_cpu.is_valid(), static_cast<const void*>(colors_cpu.ptr<float>()),
                          colors_cpu.shape().str(), colors_cpu.numel());

                auto fused_color = rgb_to_sh(colors_cpu);
                LOG_DEBUG("    fused_color: is_valid={}, shape={}, numel={}",
                          fused_color.is_valid(), fused_color.shape().str(), fused_color.numel());

                // Create SH tensor on CPU
                auto shs_cpu_tensor = Tensor::zeros(
                    {fused_color.size(0), static_cast<size_t>(feature_shape), 3},
                    Device::CPU);
                LOG_DEBUG("    shs_cpu_tensor: is_valid={}, shape={}, numel={}",
                          shs_cpu_tensor.is_valid(), shs_cpu_tensor.shape().str(), shs_cpu_tensor.numel());

                auto shs_acc = shs_cpu_tensor.accessor<float, 3>();
                auto fused_acc = fused_color.accessor<float, 2>();

                for (size_t i = 0; i < fused_color.size(0); ++i) {
                    for (size_t c = 0; c < 3; ++c) {
                        shs_acc(i, 0, c) = fused_acc(i, c); // Set DC coefficient
                    }
                }

                sh0_cpu = shs_cpu_tensor.slice(1, 0, 1).contiguous();
                if (feature_shape > 1) {
                    shN_cpu = shs_cpu_tensor.slice(1, 1, feature_shape).contiguous();
                } else {
                    // sh-degree 0: create empty shN tensor [N, 0, 3]
                    shN_cpu = Tensor::zeros({shs_cpu_tensor.size(0), 0, 3}, Device::CPU);
                }
                LOG_DEBUG("  sh0_cpu: is_valid={}, ptr={}, shape={}, numel={}",
                          sh0_cpu.is_valid(), static_cast<const void*>(sh0_cpu.ptr<float>()),
                          sh0_cpu.shape().str(), sh0_cpu.numel());
                LOG_DEBUG("  shN_cpu: is_valid={}, ptr={}, shape={}, numel={}",
                          shN_cpu.is_valid(), static_cast<const void*>(shN_cpu.ptr<float>()),
                          shN_cpu.shape().str(), shN_cpu.numel());

                // Copy CPU data to direct CUDA tensors
                LOG_DEBUG("Copying CPU values to direct CUDA tensors");
                cudaError_t err;

                // Means copy
                LOG_DEBUG("  Copying means: src_ptr={}, dst_ptr={}, bytes={}",
                          static_cast<const void*>(means_cpu.ptr<float>()),
                          static_cast<void*>(means_.ptr<float>()),
                          means_cpu.numel() * sizeof(float));
                err = cudaMemcpy(means_.ptr<float>(), means_cpu.ptr<float>(),
                                means_cpu.numel() * sizeof(float), cudaMemcpyHostToDevice);
                if (err != cudaSuccess) {
                    LOG_ERROR("cudaMemcpy failed for means:");
                    LOG_ERROR("  src (CPU): is_valid={}, ptr={}, device={}, numel={}",
                              means_cpu.is_valid(), static_cast<const void*>(means_cpu.ptr<float>()),
                              means_cpu.device() == Device::CPU ? "CPU" : "CUDA", means_cpu.numel());
                    LOG_ERROR("  dst (CUDA): is_valid={}, ptr={}, device={}, numel={}",
                              means_.is_valid(), static_cast<void*>(means_.ptr<float>()),
                              means_.device() == Device::CPU ? "CPU" : "CUDA", means_.numel());
                    throw TensorError("cudaMemcpy failed for means: " + std::string(cudaGetErrorString(err)));
                }
                LOG_DEBUG("  Means copy successful");

                // Scaling copy
                LOG_DEBUG("  Copying scaling: src_ptr={}, dst_ptr={}, bytes={}",
                          static_cast<const void*>(scaling_cpu.ptr<float>()),
                          static_cast<void*>(scaling_.ptr<float>()),
                          scaling_cpu.numel() * sizeof(float));
                err = cudaMemcpy(scaling_.ptr<float>(), scaling_cpu.ptr<float>(),
                                scaling_cpu.numel() * sizeof(float), cudaMemcpyHostToDevice);
                if (err != cudaSuccess) {
                    LOG_ERROR("cudaMemcpy failed for scaling:");
                    LOG_ERROR("  src (CPU): is_valid={}, ptr={}, numel={}",
                              scaling_cpu.is_valid(), static_cast<const void*>(scaling_cpu.ptr<float>()), scaling_cpu.numel());
                    LOG_ERROR("  dst (CUDA): is_valid={}, ptr={}, numel={}",
                              scaling_.is_valid(), static_cast<void*>(scaling_.ptr<float>()), scaling_.numel());
                    throw TensorError("cudaMemcpy failed for scaling: " + std::string(cudaGetErrorString(err)));
                }
                LOG_DEBUG("  Scaling copy successful");

                // Rotation copy
                LOG_DEBUG("  Copying rotation: src_ptr={}, dst_ptr={}, bytes={}",
                          static_cast<const void*>(rotation_cpu.ptr<float>()),
                          static_cast<void*>(rotation_.ptr<float>()),
                          rotation_cpu.numel() * sizeof(float));
                err = cudaMemcpy(rotation_.ptr<float>(), rotation_cpu.ptr<float>(),
                                rotation_cpu.numel() * sizeof(float), cudaMemcpyHostToDevice);
                if (err != cudaSuccess) {
                    LOG_ERROR("cudaMemcpy failed for rotation:");
                    LOG_ERROR("  src (CPU): is_valid={}, ptr={}, numel={}",
                              rotation_cpu.is_valid(), static_cast<const void*>(rotation_cpu.ptr<float>()), rotation_cpu.numel());
                    LOG_ERROR("  dst (CUDA): is_valid={}, ptr={}, numel={}",
                              rotation_.is_valid(), static_cast<void*>(rotation_.ptr<float>()), rotation_.numel());
                    throw TensorError("cudaMemcpy failed for rotation: " + std::string(cudaGetErrorString(err)));
                }
                LOG_DEBUG("  Rotation copy successful");

                // Opacity copy
                LOG_DEBUG("  Copying opacity: src_ptr={}, dst_ptr={}, bytes={}",
                          static_cast<const void*>(opacity_cpu.ptr<float>()),
                          static_cast<void*>(opacity_.ptr<float>()),
                          opacity_cpu.numel() * sizeof(float));
                err = cudaMemcpy(opacity_.ptr<float>(), opacity_cpu.ptr<float>(),
                                opacity_cpu.numel() * sizeof(float), cudaMemcpyHostToDevice);
                if (err != cudaSuccess) {
                    LOG_ERROR("cudaMemcpy failed for opacity:");
                    LOG_ERROR("  src (CPU): is_valid={}, ptr={}, numel={}",
                              opacity_cpu.is_valid(), static_cast<const void*>(opacity_cpu.ptr<float>()), opacity_cpu.numel());
                    LOG_ERROR("  dst (CUDA): is_valid={}, ptr={}, numel={}",
                              opacity_.is_valid(), static_cast<void*>(opacity_.ptr<float>()), opacity_.numel());
                    throw TensorError("cudaMemcpy failed for opacity: " + std::string(cudaGetErrorString(err)));
                }
                LOG_DEBUG("  Opacity copy successful");

                // SH0 copy
                LOG_DEBUG("  Copying sh0: src_ptr={}, dst_ptr={}, bytes={}",
                          static_cast<const void*>(sh0_cpu.ptr<float>()),
                          static_cast<void*>(sh0_.ptr<float>()),
                          sh0_cpu.numel() * sizeof(float));
                err = cudaMemcpy(sh0_.ptr<float>(), sh0_cpu.ptr<float>(),
                                sh0_cpu.numel() * sizeof(float), cudaMemcpyHostToDevice);
                if (err != cudaSuccess) {
                    LOG_ERROR("cudaMemcpy failed for sh0:");
                    LOG_ERROR("  src (CPU): is_valid={}, ptr={}, numel={}",
                              sh0_cpu.is_valid(), static_cast<const void*>(sh0_cpu.ptr<float>()), sh0_cpu.numel());
                    LOG_ERROR("  dst (CUDA): is_valid={}, ptr={}, numel={}",
                              sh0_.is_valid(), static_cast<void*>(sh0_.ptr<float>()), sh0_.numel());
                    throw TensorError("cudaMemcpy failed for sh0: " + std::string(cudaGetErrorString(err)));
                }
                LOG_DEBUG("  SH0 copy successful");

                // SHN copy
                LOG_DEBUG("  Copying shN: src_ptr={}, dst_ptr={}, bytes={}",
                          static_cast<const void*>(shN_cpu.ptr<float>()),
                          static_cast<void*>(shN_.ptr<float>()),
                          shN_cpu.numel() * sizeof(float));
                err = cudaMemcpy(shN_.ptr<float>(), shN_cpu.ptr<float>(),
                                shN_cpu.numel() * sizeof(float), cudaMemcpyHostToDevice);
                if (err != cudaSuccess) {
                    LOG_ERROR("cudaMemcpy failed for shN:");
                    LOG_ERROR("  src (CPU): is_valid={}, ptr={}, numel={}",
                              shN_cpu.is_valid(), static_cast<const void*>(shN_cpu.ptr<float>()), shN_cpu.numel());
                    LOG_ERROR("  dst (CUDA): is_valid={}, ptr={}, numel={}",
                              shN_.is_valid(), static_cast<void*>(shN_.ptr<float>()), shN_.numel());
                    throw TensorError("cudaMemcpy failed for shN: " + std::string(cudaGetErrorString(err)));
                }
                LOG_DEBUG("  SHN copy successful");

                LOG_DEBUG("All CPU to CUDA copies completed successfully");
            } else {
                // No capacity specified - use pool
                Tensor means_temp;
                if (params.optimization.random) {
                    means_temp = positions.mul(scene_scale).cuda();
                } else {
                    means_temp = positions.cuda();
                }

                auto nn_dist = compute_mean_neighbor_distances(means_temp).clamp_min(1e-7f);
                std::vector<int> scale_expand_shape = {static_cast<int>(num_points), 3};
                auto scaling_temp = nn_dist.sqrt()
                                    .mul(params.optimization.init_scaling)
                                    .log()
                                    .unsqueeze(-1)
                                    .expand(std::span<const int>(scale_expand_shape))
                                    .cuda();

                auto ones_col = Tensor::ones({num_points, 1}, Device::CUDA);
                auto zeros_cols = Tensor::zeros({num_points, 3}, Device::CUDA);
                auto rotation_temp = ones_col.cat(zeros_cols, 1);

                auto opacity_temp = Tensor::full({num_points, 1}, params.optimization.init_opacity, Device::CUDA).logit();

                auto colors_device = colors.cuda();
                auto fused_color = rgb_to_sh(colors_device);

                auto shs = Tensor::zeros({fused_color.size(0), static_cast<size_t>(feature_shape), 3}, Device::CUDA);
                auto shs_cpu_tmp = shs.cpu();
                auto fused_cpu_tmp = fused_color.cpu();

                auto shs_acc = shs_cpu_tmp.accessor<float, 3>();
                auto fused_acc = fused_cpu_tmp.accessor<float, 2>();

                for (size_t i = 0; i < fused_color.size(0); ++i) {
                    for (size_t c = 0; c < 3; ++c) {
                        shs_acc(i, 0, c) = fused_acc(i, c);
                    }
                }

                shs = shs_cpu_tmp.cuda();
                auto sh0_temp = shs.slice(1, 0, 1).contiguous();
                Tensor shN_temp;
                if (feature_shape > 1) {
                    shN_temp = shs.slice(1, 1, feature_shape).contiguous();
                } else {
                    // sh-degree 0: create empty shN tensor [N, 0, 3]
                    shN_temp = Tensor::zeros({shs.size(0), 0, 3}, Device::CUDA);
                }

                means_ = means_temp;
                scaling_ = scaling_temp;
                rotation_ = rotation_temp;
                opacity_ = opacity_temp;
                sh0_ = sh0_temp;
                shN_ = shN_temp;
            }

            std::println("Scene scale: {}", scene_scale);
            std::println("Initialized SplatData with:");
            std::println("  - {} points", num_points);
            std::println("  - Max SH degree: {}", params.optimization.sh_degree);
            std::println("  - Total SH coefficients: {}", feature_shape);
            std::println("  - sh0 shape: {}", sh0_.shape().str());
            std::println("  - shN shape: {}", shN_.shape().str());
            std::println("  - Layout: [N, channels={}, coeffs]", sh0_.size(1));

            auto result = SplatData(
                params.optimization.sh_degree,
                std::move(means_),
                std::move(sh0_),
                std::move(shN_),
                std::move(scaling_),
                std::move(rotation_),
                std::move(opacity_),
                scene_scale);

            return result;

        } catch (const std::exception& e) {
            return std::unexpected(
                std::format("Failed to initialize SplatData: {}", e.what()));
        }
    }

} // namespace lfs::core
