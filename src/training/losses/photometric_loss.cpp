/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 *
 * SPDX-License-Identifier: GPL-3.0-or-later */

#include "photometric_loss.hpp"
#include "lfs/kernels/ssim.cuh"
#include "lfs/kernels/l1_loss.cuh"
#include <format>

namespace lfs::training::losses {

void PhotometricLoss::ensure_buffers(const std::vector<size_t>& shape, size_t num_blocks) {
    // Only reallocate if shape or num_blocks changed
    if (allocated_shape_ != shape || allocated_num_blocks_ != num_blocks) {
        lfs::core::TensorShape tshape(shape);
        grad_buffer_ = lfs::core::Tensor::empty(tshape, lfs::core::Device::CUDA);
        loss_scalar_ = lfs::core::Tensor::zeros({1}, lfs::core::Device::CUDA);

        if (num_blocks > 0) {
            l1_reduction_buffer_ = lfs::core::Tensor::empty({num_blocks}, lfs::core::Device::CUDA);
        }

        allocated_shape_ = shape;
        allocated_num_blocks_ = num_blocks;
    }
}

std::expected<std::pair<lfs::core::Tensor, PhotometricLoss::Context>, std::string>
PhotometricLoss::forward(
    const lfs::core::Tensor& rendered,
    const lfs::core::Tensor& gt_image,
    const Params& params) {
    try {
        // Ensure 4D shape [N, C, H, W] by adding batch dimension if needed
        auto rendered_4d = rendered.ndim() == 3 ? rendered.unsqueeze(0) : rendered;
        auto gt_4d = gt_image.ndim() == 3 ? gt_image.unsqueeze(0) : gt_image;

        // Validate shapes
        if (rendered_4d.shape() != gt_4d.shape()) {
            return std::unexpected("Shape mismatch: rendered and gt_image must have same shape");
        }

        lfs::core::Tensor grad_combined;
        lfs::core::Tensor loss_tensor_gpu;

        // Optimize: only compute what's needed based on lambda_dssim
        if (params.lambda_dssim == 0.0f) {
            // Pure L1 loss - use pre-allocated buffers (eliminates ~20MB allocation churn per iteration)
            size_t N = rendered_4d.numel();
            size_t num_blocks = std::min((N + 255) / 256, size_t(1024));

            // Ensure buffers are sized correctly
            ensure_buffers(rendered_4d.shape().dims(), num_blocks);

            lfs::training::kernels::launch_fused_l1_loss(
                rendered_4d.ptr<float>(),
                gt_4d.ptr<float>(),
                grad_buffer_.ptr<float>(),
                loss_scalar_.ptr<float>(),
                l1_reduction_buffer_.ptr<float>(),
                N,
                nullptr);

            grad_combined = grad_buffer_;
            loss_tensor_gpu = loss_scalar_;

        } else if (params.lambda_dssim == 1.0f) {
            // Pure SSIM loss - skip L1 computation entirely (use pre-allocated workspace)
            auto [ssim_value_tensor, ssim_ctx] = lfs::training::kernels::ssim_forward(
                rendered_4d, gt_4d, ssim_workspace_, /*apply_valid_padding=*/true);

            // Compute loss on GPU: loss = 1 - ssim (NO CPU SYNC!)
            loss_tensor_gpu = lfs::core::Tensor::full({1}, 1.0f, lfs::core::Device::CUDA) - ssim_value_tensor;

            // Backward: d(loss)/d(ssim) = -1 (since loss = 1 - ssim)
            grad_combined = lfs::training::kernels::ssim_backward(ssim_ctx, ssim_workspace_, -1.0f);

        } else {
            // Combined loss - use pre-allocated buffers (eliminates ~40MB allocation churn per iteration)
            size_t N = rendered_4d.numel();
            size_t num_blocks = std::min((N + 255) / 256, size_t(1024));

            // Ensure buffers are sized correctly
            ensure_buffers(rendered_4d.shape().dims(), num_blocks);

            // L1 component - use pre-allocated buffers
            lfs::training::kernels::launch_fused_l1_loss(
                rendered_4d.ptr<float>(),
                gt_4d.ptr<float>(),
                grad_buffer_.ptr<float>(),
                loss_scalar_.ptr<float>(),
                l1_reduction_buffer_.ptr<float>(),
                N,
                nullptr);

            // SSIM component (use pre-allocated workspace)
            auto [ssim_value_tensor, ssim_ctx] = lfs::training::kernels::ssim_forward(
                rendered_4d, gt_4d, ssim_workspace_, /*apply_valid_padding=*/true);

            // Compute SSIM loss on GPU: loss = 1 - ssim (NO CPU SYNC!)
            // Note: Tensor::full still allocates, but it's a scalar (4 bytes)
            auto ssim_loss_tensor = lfs::core::Tensor::full({1}, 1.0f, lfs::core::Device::CUDA) - ssim_value_tensor;

            // Backward: d(loss)/d(ssim) = -1 (since loss = 1 - ssim)
            auto grad_ssim = lfs::training::kernels::ssim_backward(ssim_ctx, ssim_workspace_, -1.0f);

            // Combine gradients in-place (eliminates 2 temporary allocations per iteration)
            // grad = (1 - lambda) * grad_l1 + lambda * grad_ssim
            // grad_buffer_ is reused from L1, grad_ssim is temporary from SSIM backward
            grad_buffer_.mul_(1.0f - params.lambda_dssim);
            grad_ssim.mul_(params.lambda_dssim);
            grad_buffer_.add_(grad_ssim);
            grad_combined = grad_buffer_;

            // Combine losses in-place on GPU (eliminates 2 temporary allocations per iteration)
            // loss_scalar_ is reused from L1, ssim_loss_tensor is temporary
            loss_scalar_.mul_(1.0f - params.lambda_dssim);
            ssim_loss_tensor.mul_(params.lambda_dssim);
            loss_scalar_.add_(ssim_loss_tensor);
            loss_tensor_gpu = loss_scalar_;
        }

        // Remove batch dimension if input was 3D
        if (rendered.ndim() == 3) {
            grad_combined = grad_combined.squeeze(0);
        }

        Context ctx{
            .loss_tensor = loss_tensor_gpu,
            .grad_image = grad_combined
        };
        return std::make_pair(loss_tensor_gpu, ctx);
    } catch (const std::exception& e) {
        return std::unexpected(std::format("Error computing photometric loss with gradient: {}", e.what()));
    }
}

} // namespace lfs::training::losses
